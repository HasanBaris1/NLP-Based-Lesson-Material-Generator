 Thank you, everyone. I think this is going to be a fun talk and a promising talk. And when I say promising, what I mean is I'm going to promise you at the beginning what my conclusions are going to be. And then we'll come back to them at the end and see if I've kept that promise. So the most important point I want to get across is that while you hear about machine learning all the time, ubiquitously, there really are two possible power sources that can drive an artificial intelligence. And they really come from different places, one effectively from statistics and one from logic. The second point is that each of them have pluses and minuses. They each have strengths and weaknesses. And you don't have to choose between them. You can actually build AI programs that incorporate both of those sources of power. So all that and a couple demos and more. And we'll see if I keep my promise by the end of the talk. So when I talk about there being two different power sources, you can think of them almost like the two hemispheres of your brain, where your right brain hemisphere does a kind of pattern formation, very much like what neural nets do, very much like what machine learning does, where you get the equivalent of patterns built up and used when you encounter new data, sort of like when Amazon or Netflix makes recommendations about products or movies for you, using multilayer neural nets trained on big data. And when I talk about left brain thinking, I'm talking more about like what the Sherlock Holmes character does, metaphorically puffing on his Mierscham pipe, thinking about clues and cues and putting together step by step by step logical arguments that plausibly lead him to various conclusions. Using rules of thumb like ownership transfers through physical parts. So since you own that shirt, I'm going to assume that you own the buttons of that shirt. Since you own a certain car, you probably own the left front tire of that car and so on. So that's a rule of thumb, a rule of common sense reasoning. And there are actually tens of millions of those rules of common sense reasoning that we typically have formed by about the time we reach age three or four. So about the time that we get really linguistic abilities, we've already developed this layer of common sense that will serve us in good stead for the rest of our lives. And it's not a coincidence that evolution has continued to maintain this bicameral brain structure in human beings. If it weren't cost effective, evolution would have atrophied our left brain hemispheres a long time ago. So as we'll see, just with human beings, AI programs can get good value out of both left brain and right brain thinking. Now there is, to be sure, a kind of brittleness in right brain AIs. And I could give you examples of that, like yes, a program like Amazon's might recommend because you liked one of these books, you might like the other, whereas your friend who knows that you recently lost a child, and that's one of the main themes of that second book, might go out of their way to not recommend that book to you. That kind of right brain brittleness is there every time we talk to our assistants, Alexa and Google Assistant and so on. That kind of brittleness is there every time we use a search engine like Google. In this case, we're asking who was the prime minister of the United Kingdom when the current prime minister was born? If you ask that, you'll get a million hits that tell you when Theresa May was born, none of which mention the actual answer, which is Anthony Eden, which is very sad. Because of course, if you plug in that date and say who was the prime minister of Great Britain in 1956, you'll then get a million hits that tell you the answer was Anthony Eden. Even that one step logical inference is not there in Google. Part of the impressiveness of Google is that it has trained us to be content with, to not expect even that kind of one step logical reasoning being provided by the machine. Now, left brain AIs are brittle in their own ways. For instance, the world's leading skin disease diagnosis system, which contains a large number of diagnosis rules. If you go through a particular case like this, it will basically, are there spots on the body? Yes. What color? Reddish brown. Are there more on the trunk than elsewhere? No. Then it will diagnose the patient as having measles. Unfortunately, this was the patient. The system didn't understand things like automobiles can't get human diseases and so on. Both the left and right brain programs have not really intelligence, but the thin veneer of intelligence, which reminds me in fact of a program that got me into AI about 40 years ago, which was the ELISA or doctor program that Joe Weizenbaum did at MIT, which played a rogerian psychiatrist. You would talk to it and you would say things like, I'm trying to quit smoking. It would say, tell me more about how you feel about trying to quit smoking and so on. One of my favorite interactions was when I said, my dog's mother died recently and it said, tell me more about your mother, which at one level is a deep Freudian insight, but you as computer scientists know that what's really going on here is the program didn't know the word dog and so it just heard blah, blah, blah, mother, and it said, tell me more about your mother. This is a lot like the Gary Larson far side cartoon of what you say to the dog and what the dog actually hears. For those of you who may be dog people, I will show you the rarely seen cat version of this, which is what you say to the cat and what the cat actually hears. But anyway, even if you look at the best chat bots today, you'll see that they have that kind of ELISA-like garbling or in this case where we ask one of the chat bots, where is Sue's nose when Sue is in her house and it says where it belongs, which is actually not a bad answer, but it then ruins it by going on and saying, try searching the internet. So anyway, that's what I mean by the veneer of intelligence. So each of these suffers from its own kind of brittleness bottleneck, but each also has enormous power. The power of statistical machine learning is the power of more being more. And sometimes you can hear this referred to as the wisdom of the crowd, which goes back to Francis Galton in the early 1900s when he was at a county fair and he noticed that there was a jar with 800 guesses for how much this ox weighed. And I think whoever guessed right got, I don't know if they got the ox or they got some prize, but anyway, what he noticed was that the average of all of the 800 guesses was actually more accurate than any of the 800 guesses. And so he formulated this sort of law, which is now referred to under his name, and which really is one of the foundations of the wisdom of the crowd of statistics of machine learning. So the power is not just that more is more, but also it's powerful because the effort to start up a machine learning solution is fairly low. You can grab one of the existing algorithms off the shelf, you can dump a large amount of data into it, and a day or two later you can be up and running and actually get useful, actionable results. So that's the power of statistics. The power of logic is a kind of trust. So it's transparent, you can actually see the step-by-step logical reasoning, and unlike statistical connections, is related to links. Even if you have a logical derivation which is hundreds or thousands or tens of thousands of steps long, you can still trust the result. So if the demo gods don't conspire against this, I can actually stop this and give you a short demonstration of what I'm talking about here. So let me go ahead and do that. So this is a program called PSYCH, CYC, that as John said, our company has been developing over the last few decades. And here we've just, for fun, asked it a bunch of questions, and so you can ask things like, oh, how flammable is a, let's pick a good artifact, let's say, table. So how flammable is a table? And the answer, oh, that's not good. So one of the common sense heuristics that we develop in our culture is if something isn't working, unplug it and plug it back in. And fortunately, that gets us by 80% of the time and 100% of the time in this case. So here, the system is basically saying, well, it depends what the table is made of. So that's good. So if I take another example question would be something like, can an upside down coffee cup hold coffee? And it basically says no. And if you ask why, it has to do with the fact that in general, if you turn an open container upside down, the stuff inside will fall out. Assuming that you're doing that in an environment where the stuff inside is less, is more dense than the atmosphere or the environment. So if you were doing this with a coffee cup filled with helium, the answer might be yes. Sorry, the answer would be yes. But in the case of something like a coffee cup filled with coffee or a coffee cup filled with marbles, the answer is no. But if we ask even more complicated questions, something like involving conceptual works, for instance, questions about Romeo and Juliet, even complicated questions like when Juliet takes the apothecary's potion that puts her into a state of suspended animation, does she believe that Romeo will believe that she's alive during that period of time even after other people tell him that she's dead? And the answer is yes. And again, psych is able to give you the step-by-step reasoning. Now all this English is generated in real time as we're sitting here by the system from the underlying logical representation. So all of this is represented in logic, which takes about usually about one and a half or twice as many words as it were to say as it said in English. So this is basically the fact that Juliet believes that at the time that she's under this spell, Romeo will actually believe that she's still alive. In this case, it's because she believes that because they've planned something, the plans will happen and that Romeo will have received a message that she's still alive even during the time that she's in suspension and so on. Or to take an even more complicated question, although that was pretty complicated because we're talking about what Juliet believes that Romeo believes about Juliet at some hypothetical future time and so on. We could basically take a question like what are the apothecary's positive and negative for selling Romeo that fatal poison that he buys near the end of Act IV. In this case, there are incentives, namely it turns out that the apothecary is extremely poor and really needs the money and there are disincentives because the law of the land in Mantua, which is where they are at the time, is that if you sell fatal poison, the punishment for that if you're caught is to be executed and in general, people don't like to be executed. Again, all of this is represented in logic and this is basically just the fact that the apothecary is subject to the code of conduct of the place he's living, which is Mantua and so on. I could go on with that. Let me give you a different kind of demo though of the system doing an actual application. This was something we did with Accenture for South by Southwest. Thousands of people come to Austin, Texas for South by Southwest and there are about four or five hundred things to do in Austin, Texas and instead of just relying on something like Yelp or other reviews, they thought it would be nice to have Psych actually model the person. In this case, they're modeling. They take a couple minutes and they model each person and then come up with suggestions, customized suggestions for what they might want to do. In this case, for instance, the suggestion is there are about ten suggestions and one of which is Hamilton Pool and Hamilton Pool is like number nine on the top ten list and it has a couple of con reasons because it's not dog friendly and you said you wanted to bring your dog and because you're liable to slip and fall, it's like what the hell? Why are you liable to slip and fall? So if you dig into this, you can basically get Psych to tell you the reasons why, which basically are why am I likely to fall? Well, it's because the trails are going to be slippery because there's an 80 to 100% chance of rain in 24 hours right before you're going to be visiting it and the trails there are slippery and you probably didn't bring proper foot gear with you because you're traveling and you live outside Austin and so on. So at any given point, we can reject any of these like I can say I actually did bring the right thing with me and then have the system recalculate its recommendations and then Hamilton Pool would be higher on the list of recommendations. So I think you get the gist of that one. Okay. So we can do more demos or questions at the end if you're interested and if we have time. So what are the strengths and weaknesses? What are the places where current machine learning technology really falls down where the symbolic kind of AI that I was just showing you could actually help? So one of the clearest types of examples of those are where there's very little training data. There isn't big data available. An example is an application we did for a large oil company and one of the conditions of interest they wanted the system to detect were when there was about to be a containment failure and essentially a blowout was about to happen and so on. Well if you have a lot of training data for your system, you probably aren't still in business because that's the kind of thing that should only happen like zero or one or two times in the history of your company and so on. So there are very important cases where the training data just isn't there. Another case is where there are two or more faults going on at once. When you're doing diagnosis, it's very common to make the one fault assumption which is hope there's only one thing going wrong like we did with unplugging and replugging in the cable there. If you have two things going wrong at once, it's often very, very hard to figure out what's going on, especially if they superpose like one medical condition raises your temperature and one medical condition lowers your temperature so it looks like your temperature is normal or something. So this is an example of a program that Psych Power is called Mathcraft which helps sixth graders and fifth graders doing math, learning math and in this case, very simple linear equation and they have to solve it, a sixth grade problem and they get this horrible answer instead of the correct answer which is four, they get 213. Because people use calculators so much, they don't even realize what a terrible answer that is. So how in the world did they get 213 as a wrong answer to that? It would take you a long time, I think, to try to figure out what they did wrong. If you watch them do their work, you can see the first step they did was right which is to add 34 to both sides of the equation but then they did something bad which was they thought they got from 11X to X by subtracting 11 so instead of dividing 44 by 11, they subtracted 11 from 44 and then they made another mistake when they were doing that subtraction, they borrowed when they didn't have to. So they did something like that, now they weren't sure what to do so they did like 14 minus 1 is 13 and 3 minus 1 is 2 and that's how they got 213. So the psych system models the person who is the user of the application and is able to figure out what one or two or three things they might have done wrong, plausibly not really deducing it so much as abducing or guessing what they might have done wrong and then figures out how to remediate that and as we saw before is able to do fairly good natural language generation from the logic so the English explanation if there's a human teacher comes out looking in a readable fashion. Now this application, Mathcraft, was actually part of an interesting, almost paradigm changing Unity application that we did for the sixth graders and effectively it's based on this phenomenon you've probably all experienced in your life where you thought you understood something but then you only really understood it after you had to teach it or explain it to somebody else. We've all had that experience and yet despite the ubiquity of that experience almost all of the educational software that's out there in the world has the computer playing the role of the teacher and has the user playing the role of the student. So in Mathcraft the roles are reversed and in fact your job is not to solve math problems, it's to watch this cute little avatar struggling to survive in a crashed spaceship as he or she solves math problems and you give them advice about what they're doing right or what they're doing wrong and if you give them good advice about the problems then they're able to solve the problems in their environment like this stuck door and so on and if you give them good advice then the model of what you understand gets updated and as a result of that Psych doesn't have the avatar make that mistake very much anymore and as a result of that you get this engagement because you feel like you've taught the little avatar something so it's a very different sort of educational paradigm that could be used and is being used in training not just for kids but also for military applications and for some commercial applications. So another canonical kind of place where machine learning has trouble is situations that haven't happened yet. There's a policy decision you're considering and you want to know what the impact of that is but you haven't made that policy decision yet. What will happen if I hook this up to my network? I haven't hooked it up yet. I don't have the data yet or I want to do a counterterrorism scenario so I want to imagine bad outcomes and what series of events could have brought them about and keep hypothesizing things back in time until I get to things which I can actually check on and a lot of been effectively helping to keep the world safer by out thinking or pre-thinking terrorist attacks and this is one of the major ways in which the government is able to avoid confirmation bias and think out of the box and cover more future possibilities. So instead of referring to it as ML suffers let's put a more euphemistic way of putting it that ML could benefit by partnering with symbolic logic based AI if the things we've talked about already there's little or no data because of rarity or the multi fault hypothesis happening or because it's a counterfactual future conditional event or seeing the step by step argument is important. One of the things that really had me start on this journey about 36 years ago was when my daughter who was very young at the time she's fine now was diagnosed as having meningitis and when we took her into Stanford Hospital I could not get a straight answer from the doctors about why they thought she had meningitis about what the possibilities were about what they wanted to do the procedures they wanted to do and why and so on it was extremely frustrating and my friend Ted Shortliff who was a professor along with me there at that time was working on a program called MISON a medical diagnosis program and we put her case through MISON it came up with exactly the same diagnosis exactly the same therapy recommendations but the difference was that MISON because it was a rule-based logical system was able to show us step by step by step what the argument was what the reasoning was behind it and that really made all the difference in the world for my wife and myself to feel comfortable with what was going on and that was a kind of epiphany for me and that really showed me that in addition to being right it's often important to understand the argument behind what it is that you're saying often we don't just want one right answer we want the plausible the most likely answers and an argument for and against each one of them that's true in medical diagnosis it's true in counterintelligence reasoning for instance here when the Madrid train station bombing happened many intelligence agencies across the world jumped to the conclusion that ETA was responsible ETA is a separatist Basque terrorist group that is pretty much responsible for most of the bad stuff that has happened in Spain in the last several decades but psych was able to come up with a series of pro and con arguments for why it might or might not have been ETA why it might or might not have been Al Qaeda and so on and based on that US intelligence and Israeli intelligence did not jump to that wrong conclusion and in fact in hindsight it turned out it was Al Qaeda that was responsible the same kind of thing happens if you have a problem which multiple experts would have their own opinions on and it's not like you want to listen to any one of them to make the most informed decision you really want to hear all of them arguing with each other and then make your own decision and there's no reason why symbolically you can't separately model each of those individuals and what advice or what conclusions they would have given so in many cases the person who's best at some task is much much much better than the average person at that task one common reason this is true is tasks where you have very high turnover for example on oil platforms in the Gulf of Mexico where anyone who gets enough seniority wants to transfer off the oil platform as quickly as possible or in some high tech fields where your experts are getting poached by competition or things like that another common case is where the user thinks they have a question they want to ask but in fact there are often several iterations involved so an example we did for the Cleveland Clinic where their experts doing clinical trial cohort selection would come up with a question they would articulate it to a nurse the nurse would talk to a database person who would write some customized sequel and sparkle a few days later that would come back the doctor would look at the results and say oh well obviously I meant blah blah blah and they'd modify their question and that would go around and around typically two weeks would pass until the doctor got the answer to the question they actually had intended to ask and so in this application we were able to let them put in fairly long questions this is actually a short one but some of the questions had 40, 50, 60 words in them way beyond the level of parsing technology today but what Psych was able to do was to recognize fragments almost like fill in the blank mad lib fragments like I think your question had something to do with patients at the clinic and I think your question had something to do with 2014 and 2015 and so on by the way this is a common case where the doctor said and where they really meant or unless you're really really really unlucky you didn't really have two valve replacements in you know one in 2014 and one in 2015 so really what the doctor was asking for was patients who had this occur during either of those years not both of those years and so on but anyway a really cool thing happens once the doctor then selects which of those fragments were part of their intended question Psych brings to bear medical knowledge discourse knowledge common sense knowledge and often the number of ways that those fragments can be combined into a meaningful question is one there's only one single meaningful question the doctor could have plausibly had in mind that would have caused them to have yeses to these fragments and no's to those fragments and then it puts them together generates an English paraphrase and then goes off to the database to answer those questions so I've talked a little bit about Psych so let me give you the couple minutes summary of what the technology is I've talked about the two sources of power and it's important to recognize and many of you are young enough that you may not recognize that both of these technologies have been around for depending on how you count 40 or 45 or maybe even 50 years and haven't really changed that much in the last decades so there were neural net based statistical based machine learning systems available in the 1960s and throughout the 1970s I worked on many of those while I was at Stanford in the early and mid and late 70s in fact but what they were waiting on was not so much an insight as it was they were waiting on computers to get faster storage to get cheaper big data to go from being rare to being ubiquitous and so on and yes there were one or two ideas that had to come along like convolution and so on but mostly it was just waiting for technology to catch up with the algorithm in the left brain case yes we were also waiting for computers to get faster but there were really also about 150 engineering problems that had to get solved dealing with things like default reasoning and contexts and so on and we had to prime the pump with this large knowledge base of tens of millions of general rules of common sense and so on so I'm not going to go through the 150 engineering challenges I've mentioned one or two of them already and I'll talk about one or two in the next few minutes but the idea was in about 1980 that someone should drag this mattress off the road so traffic toward general AI could proceed and I was making a lot of noise about that and Congress actually did something that's how you can tell it was several decades ago Congress basically said we're going to pass something called the National Cooperative Research Act because we're concerned about the Japanese fifth generation computer effort and so on we're going to let big companies collude we're going to let big companies work together on R&D and not prosecute them for antitrust law violations and so research consortia sprang up in the United States the first one was called MCC in Austin, Texas and Admiral Bob Inman went to run that one of the smartest people I've ever met and he came to visit me at Stanford and he said hey professor you've got like seven graduate students you're talking about a project that's going to take thousands of person years do the math it's going to take you several centuries to get this done if you move to the wilds of Austin, Texas we'll have ten times that many people working on it and you'll just barely live to see the end of it and the good news is that since you folks have waited 35 years to hear this talk we actually are at the end of that process and we do have this technology that can actually be applied which is pretty exciting so what we've built in those decades is this large collection of this large ontology of rules of thumb like ownership transfers through physical parts and causes proceed effects and if some device isn't working try turning it off and on again and things like that and when I talk about tens of millions of those I'm not talking about kind of facts that you would find on Wikipedia or that you would access in some database I'm talking about these general rules of thumb things which often aren't stated explicitly because they're things that everybody knows in fact if you Google things like water flowing uphill and water flowing downhill you get more references in the world to water flowing uphill because people are using that as a kind of dramatic metaphor they don't really talk about water flowing downhill because everybody knows that water flows downhill so why actually say that? So we've got about 42,000, 43,000 different relations or functions or predicates that connect those different terms and about a million and a half general terms not proper nouns but general terms like table and furniture and so on. So this was what I meant by priming the pump and just to show you that it really does work that it really is important to prime the pump in this way when we built that Cleveland Clinic application the amount of things we had to tell the system was down around the 100,000, 120,000 levels of things we had to tell the system but it actually used many millions of things that were in its knowledge base and in fact if you ask a particular question like this and actually see what pieces of knowledge are used in the arguments ultimately something like 95% of what it's actually using to answer your questions were things that it had known years in most cases many years before the application was even started. And speaking of common sense almost any application where humans bring common sense to bear is one where left brain thinking is relevant. This is one of the reasons I am very skeptical about how rapidly and how pervasively autonomous vehicles are going to catch on and be successful. I think they are metaphorically and literally accidents waiting to happen because as you drive around for example in Chicago as you drive around every day every week something wacky happens there is something in the road that shouldn't be there and you have to use your knowledge of it's a bag of stuff but is it McDonald's bag or is it a Home Depot bag or something like that. So what I do in terms of slamming on my brakes or swerving or just running it over depends on something I happen to know in the real world. But anyway when I talk about common sense tasks I'm talking about things like disambiguating sentences that have pronouns in them for example or polysemous ambiguous words. So if I say without even telling you what it is if it wouldn't fit in the suitcase because it was too big then you know it's the thing I'm trying to stuff in the suitcase but if I just change one word and say it wouldn't fit in the suitcase because it was too small then that second it refers to the suitcase not the thing I'm trying to stuff into it because big things don't fit into smaller things something that psych has known for about 30 years or if you look at this pair of sentences Fred was mad at Tom because he stole his lunch who stole whose lunch well presumably Fred was mad at Tom because Tom stole Fred's lunch if I just change one word here Fred was mad at Tom so presumably Fred was doing the stealing in that second case and by the way in neither case did it even cross your mind for a second that Fred was stealing Fred's lunch or that Tom was stealing Tom's lunch and so on. So anywhere you see these kinds of pronouns or ambiguous words the author or the speaker is presuming that the reader or the listener has and is going to bring to bear this body of common sense that we develop pretty much as two year olds and three year olds and four year olds. Here's another example where we're asking to find pictures that contain a situation where there's a man who's smiling and Psych is able to match this one which has a caption of someone helping his daughter take her first step and Psych doesn't actually look at the image so imagine that the image is bad quality or you only see the person the only see the adult from behind maybe you see the cute little kid from the front but you only see the back of the father who's helping her or something but still you as a human being know that the father in that picture the father is smiling even if you can't see a smiling person in the picture. How do you know that the person is smiling? It's because you have pieces of knowledge about the world like when you're happy you smile and if somebody you love accomplishes something then that makes you happy and generally parents love their children and so on and so when we axiomatize when we represent in logic each of those things and we represent in logic the captions and so on then Psych is able to very quickly find the connections even though the way it does it is completely different from the way machine learning would try to process the image in order to recognize things in the image and that's what I really am trying to get across the difference in the sources of power and the strengths and weaknesses that they have. The main weakness that logic has is that the pieces the rules have to fit together and without the general common sense background if you have narrow stove piped logic applications the rules don't generally fit together very well so part of what we're trying to do in the Psych effort is to build that semantic glue that will make those pieces fit together that will relieve one of the limitations of symbolic AIs. Now I've been talking about symbolic AI as if everybody is either doing machine learning or everybody else is doing what we're doing in Psych. In actuality they're really most of the people doing symbolic AI are doing something using a much more limited logic than the kind of logic that I was showing you involving Romeo and Juliet where humans read that or see that play and they're able to understand the plot they're able to understand who did what and why but what if we actually try and have AIs work on something like the plot of Romeo and Juliet using text processing you can answer some questions using things like named entity recognition, simple binary relation extraction, link analysis, sentiment analysis text searching, LSI, latent semantic indexing, latent semantic analysis and so on. You can answer some questions but there are an awful lot that you really need a deeper kind of logic for and what are those deeper kinds of logic? Well there are three limited kinds of logic. There's propositional logic where each of the statements is a kind of opaque there are various sorts of limited logics which are special limited versions of first order predicate calculus and first order logic. Things like knowledge graphs and triple stores and Bayesian networks and property graphs and so on and they basically can do things sort of at the level that we were doing geometry problems when we were in eighth grade where you're given some things and then you have to prove some things or in a problem like this where you have seven things that you're given and then you have to prove three different statements so a typical limited logic program today you might have thousands of givens and you might have hundreds of things that you're trying to prove. Well so what separates all those limited logics there are a set of things. One is do they actually admit universal and existential quantifiers. For instance every American has a mother and every American has a president it's the same president but it's not the same mother and so that's ambiguous in English but it's not at all ambiguous in any limited logic where you can have these quantifiers because the order of the quantifiers tells you that in the second case it's one single person who's the president in the first case it's one person who's the mother for each person who is an American and so on or can you have function symbols can you have relations that take three arguments not just two can you have relations like plus that take any number of arguments how do you deal with negation some of you may be surprised to learn that most of the symbolic logic AIs make this assumption that if you try to prove something and you fail that means that the thing you were trying to prove is false so this is like really really dangerous assumption to make and yet most of the limited logics make that. What about dealing with things that are only default true like birds can fly except penguins or birds can fly but with an exception of a bird over here that has a broken wing or something like that or things that are true in one context but false in another or how do you deal with the fact that you might believe something but you'll learn something tomorrow that causes you to want to change your belief and how do you maintain truth when you keep finding out new things and how do you do that efficiently so that's really the reason why the reason why you know why are there this plethora of different logics and everything that people are using different representations and it all comes down to efficiency that you want to be able to get answers to questions in real time or in the amount of time you have you can't wait until the heat death of the universe in order to get some proof to return and theorem provers are notoriously slow so unlike machine learning systems if you're not careful the logic half of the brain on the logic half of the AI will be extremely slow so how is it that we can speed this up so what is the trade off curve really the trade off curve is that you have lots of ways of representing things and each representation lets you express more complicated things but is less efficient for the machines to reason with so natural languages are extremely expressive that's why we talk in in languages but they're very difficult for computers to really get much traction on and do much with by contrast computer programming languages are very difficult like how would we express Romeo and Juliet in prologue or in a triple store how would you express Romeo and Juliet in three word sentences theoretically it might be possible but it wouldn't be a pleasant experience for whoever had to write it or whoever had to read it if you really tried to take something like that in that way so what we want is something up and to the right but there is nothing up and to the right there is no representation which is both efficient for computer programs and expressive in terms of what can be captured but the problem is even worse because most of the time even full first order logic is not enough if you look at almost any real communications among people you find things which involve modals and other complicated things like you know being able to talk about the sentences themselves being able to ask what relationships occur but hold between two things where now one of the variables is the relationship symbol or the predicate symbol and so on and so I could go on and on and on about not just which of these have the right answers but also why are each of these true or if you look at almost any news article or listen to any news story on TV or on the web or something this is about Rod Rosenstein resigning yesterday but the point is that they're filled with the assumption that you the reader or you the listener will have no trouble hearing about things where person X at some time believed that person Y at some other time wanted them to believe that something was true dot dot dot and you have no trouble expressing that understanding it processing it but those things go way beyond what you can do in first order logic there are higher order logics which are understood by philosophers and have been for many years and the trouble is that they're not extremely slow except part of our focus in the last 35 years has been finding ways to speed that up so in particular we speed that up by separating the epistemological problem of what should the system know from the heuristic problem of how can the system know what the system is going to do and how can we do that and we map between them and that's really what I meant when I said we had not just one theorem prover we have 1100 reasoning engines 1100 different inference engines that are all working on every problem and every sub problem and they raise their hand and they're all working on the same problem and we map between them and that's really what I meant when I said we had not just one theorem prover we have 1100 reasoning engines 1100 different inference engines that are all working on every problem and every sub problem and they raise their hand when they can make some progress and so as a community of agents they work together to tackle problems some of those reasoners are very general like caching any kind of transitive relation cash the transitive closure or the cleanest are that relation so it's a redundant representation that enables a special purpose algorithm to work really really really quickly so if I want to know whether Donald Trump is transparent or opaque I don't have to do some seven step graph walk in an ontology I can just look it up in the cashed table for that relation and get the answer some of the 1100 are very domain specific like how to balance aqueous chemistry equations and so on but all together we're adding a few of these every week and so this set of agents is growing and together they're able to efficiently solve it and if we reach an application where the system gets the right answer but it takes too long you just talk to the human experts and you say hey how were you able to do this in a few seconds when it took our program 100 times and you're able to solve it so that's another way of getting efficiency is to have meta reasoning and meta meta reasoning to do tactical and strategic reasoning about the reasoning that's going on another way of getting efficiency is to have context because often when you're solving a problem in some context you can ignore it if you're not sure relevant information so if I ask you whether say Barack Obama is standing or sitting right now after a second or so you'll realize that you don't know the answer to that question and you don't sort of get stuck bringing in like well how many legs does a spider have and so on you know very very quickly that you don't know the answer to that question and you're able to not get caught in those time consuming loops so what did I mean when I said that these things could work together there's some uninteresting ways that they can work together where a system let's say that recognizes cats when it recognizes the cat can hand a message off to a symbolic AI saying hey there's a cat in this picture or something like that there's a more interesting case still not the most interesting one a more interesting one where we can take the natural language generation capability that I showed you and we can actually generate variant English sentences for every logical assertion and we can then use that as training data for the machine learning systems and there's actually a project that's starting up in a month at DARPA called machine common sense where all the participants are machine learning companies and universities doing machine learning except for us and we're generating this massive amount of training data for these machine learning systems so the third and this is really the interesting and important way that the systems can work together is where the machine learning system generates some hypothesis and then hands that off to the symbolic AI system to say is this consistent with everything that you know about the world and obviously if the answer is no then you shouldn't trust the result of what it just came up with but even better if the symbolic system can come up with alternate possible causal pathways causal explanations for how that might have come about so this is a project we did for NLM the National Library of Medicine one of the National Institutes of Health and it was again something we did with the Cleveland Clinic and some universities here and the idea is that in in the hospitals nowadays when you come in and check in to a good hospital since the cost of sequencing your DNA has decreased from infinity to billions of dollars down to just hundreds of dollars it's actually quite common to just sequence every patient's DNA as they come in and so then you can find the snips the single nucleotide polymorphisms the point mutations in that patient's DNA and you can build up a big database of correlations between the medical condition that they came into the hospital with and the mutations in their genome and those genome wide association studies or GWASs have gotten built up doctors were originally very excited about this have since become very much cooler about them because a lot of what they say is just many of these A to Z correlations turn out not to actually be worth the time to explore so what does psych do? Psych comes in and it has models of what polymerizes what, what catalyzes what, what reactions happen where in the body we have access to a billion triples on linked life data about things like this and psych puts together five, ten, twenty, thirty step causal chain that would go from that single nucleotide polymorphisms to the condition like osteoporosis that brought you into the hospital so psych might put together some chain that says oh yeah that mutation is next to this gene which if it were expressed would be this protein which if it got to this part of the body would catalyze this reaction you'd have more bioactive vitamin D in your blood by the way blah, blah, blah, blah, twenty steps later that screws up bone resorption and that's why you got osteoporosis so you shouldn't really believe those causal chain hypotheses anymore than you believe the A to Z correlations but if you can't even come up with one causal explanation then you should be very distrustful of the A to Z correlation but even more excitingly if you can make independent predictions as you go through that process you can then go back to the data to confirm or disconfirm those predictions like in this case we were predicting that there would be more bioactive vitamin D in these patients blood and so we can go back to the data to confirm or disconfirm that so in some ways this is the most important slide I have in the whole presentation I think it basically is the one which is communicating the way that I see this linkage happening where you generate hypotheses using machine learning you generate possible explanations using symbolic reasoning you go back to the data to confirm or disconfirm the predictions that would be inherent in those causal pathways so this is really the difference between correlation and causation and since you've been a good audience I'm nearing the end of my talk so I'll give you as a treat a couple of my favorite high correlation examples that are almost certainly not real causation ones like this one between the divorce rate in Maine and the amount of margarine that was eaten and this is my favorite one because secretly we secretly we'd like there to be a causal connection here where you know as people used Internet Explorer less and less the murder rate has gone down and and so on so I guess my closing observation will be that in some ways of statistics and symbolic reasoning are just different evolutionary stages not really so much competing ideologies you know for a hundred thousand years people noticed that if a couple who didn't have red hair had any children that had red hair about one quarter of their children had red hair and it's only fairly recently when we've understood dominant and recessive genes why we understand why that's true and why it should be exactly one quarter and so on in seven hundred years ago we had Giotto painting pictures with perspective but not perfectly because he didn't really understand what he was doing he could only impart it to apprentices through months and years of study but a hundred years later thanks to Brunelleschi and so on we understood the principles behind perspective bridge building is another case where you might think the bridge building is well understood but even less than a hundred years ago you know horrible accidents like like this took place and even last year just outside of Miami there was this bridge collapse that killed several people and so on or another example is going all the way back you know it's good that Copernicus didn't have 1960 or beyond computing technology or he would never have had to you know figure out the heliocentric model of the solar system because you could just calculate at the cycles well enough that would agree with all telescopic observations or when this phenomenon happened where like how many of you people see this as white and gold hands okay how many of you see this as blue and black yeah so about 70% of people see this as blue and black I'm one of the people who see this as white and gold about 30% of us do and gradually over the weeks and months people began to figure out causally why this was true and understand that causally and so on so you can think of causality and statistics as sort of like this infinite series of elephants and turtles and I I promised you there would be fireworks it's a little bit of fireworks at the end and so on but anyway I hope that you got out of it the things that I promised that you would get out of it and that you'll think when you're building a eyes in the future of not just using machine learning but actually building and using a eyes that have both halves of their brains functioning thank you.